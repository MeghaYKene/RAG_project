A Retrieval-Augmented Generation (RAG) pipeline built with semantic chunking, leveraging LangChain, Hugging Face models, Pinecone vector database, and Groq LLM for accurate and 
context-aware responses.

Overview
This project implements a domain-specific chatbot that can respond to user queries by retrieving relevant information from a custom knowledge base and generating human-like responses using an LLM.

Key Highlights:

ðŸ”¹ Semantic chunking for context-preserving document splitting

ðŸ”¹ Embedding and similarity search with Pinecone

ðŸ”¹ LLM inference via Groq for fast generation

ðŸ”¹ LangChain used to chain components in a modular pipeline

ðŸ”¹ Open-source models from Hugging Face
